{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter11-activity1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37vrnAFPW6_C",
        "colab_type": "text"
      },
      "source": [
        "#Activity:  Preparing Speed Dating Dataset\n",
        "\n",
        "As an entrepreneur, you are planning to launch a new dating app into the market. The key feature that will differentiate your from other competitors will be your high performing matching algorithm between users. Before starting to build this model, you partnered with a speed dating company to collect some data from real events. You just received the dataset from your partner but realised it is not as clean as expected. So your task is to fix the main data quality issues you will find.\n",
        "The following steps will help you complete this activity:\n",
        "Download and load the dataset into Python\n",
        "Check for duplicated rows\n",
        "Check for unexpected values for numerical variables\n",
        "Check for incorrect data type\n",
        "Check for missing values\n",
        "Fix identified issues if needed\n",
        "The original dataset has been shared by Ray Fisman and Sheena Iyengar from Columbia Business School: \n",
        "\n",
        "http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/\n",
        "\n",
        "The authors have provided a very useful document describing the dataset and its features: \n",
        "\n",
        "http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/Speed%20Dating%20Data%20Key.doc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ox-pHUPX78-",
        "colab_type": "text"
      },
      "source": [
        "1. Open on a new Colab notebook and import the pandas package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEiOAwQPW0qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6NbwMIwX_84",
        "colab_type": "text"
      },
      "source": [
        "2. Assign the link to the dataset to a variable called 'file_url':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7whidfaYjns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_url = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter11/dataset/Speed_Dating_Data.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzoH_wvLuD0p",
        "colab_type": "text"
      },
      "source": [
        "3. Using the read_csv method from the package pandas, load the dataset into a new variable called 'df':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFMz2jNVt-xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(file_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "garzi94On5x9",
        "colab_type": "text"
      },
      "source": [
        "4. Print the first 5 rows of the dataframe using the method .head():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xY9ATvN6-M",
        "colab_type": "code",
        "outputId": "9a131ce2-bea3-4af9-e3c3-af6a42e55f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>positin1</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>match</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>like_o</th>\n",
              "      <th>prob_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>undergra</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>race</th>\n",
              "      <th>...</th>\n",
              "      <th>amb5_2</th>\n",
              "      <th>you_call</th>\n",
              "      <th>them_cal</th>\n",
              "      <th>date_3</th>\n",
              "      <th>numdat_3</th>\n",
              "      <th>num_in_3</th>\n",
              "      <th>attr1_3</th>\n",
              "      <th>sinc1_3</th>\n",
              "      <th>intel1_3</th>\n",
              "      <th>fun1_3</th>\n",
              "      <th>amb1_3</th>\n",
              "      <th>shar1_3</th>\n",
              "      <th>attr7_3</th>\n",
              "      <th>sinc7_3</th>\n",
              "      <th>intel7_3</th>\n",
              "      <th>fun7_3</th>\n",
              "      <th>amb7_3</th>\n",
              "      <th>shar7_3</th>\n",
              "      <th>attr4_3</th>\n",
              "      <th>sinc4_3</th>\n",
              "      <th>intel4_3</th>\n",
              "      <th>fun4_3</th>\n",
              "      <th>amb4_3</th>\n",
              "      <th>shar4_3</th>\n",
              "      <th>attr2_3</th>\n",
              "      <th>sinc2_3</th>\n",
              "      <th>intel2_3</th>\n",
              "      <th>fun2_3</th>\n",
              "      <th>amb2_3</th>\n",
              "      <th>shar2_3</th>\n",
              "      <th>attr3_3</th>\n",
              "      <th>sinc3_3</th>\n",
              "      <th>intel3_3</th>\n",
              "      <th>fun3_3</th>\n",
              "      <th>amb3_3</th>\n",
              "      <th>attr5_3</th>\n",
              "      <th>sinc5_3</th>\n",
              "      <th>intel5_3</th>\n",
              "      <th>fun5_3</th>\n",
              "      <th>amb5_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 195 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   iid   id  gender  idg  condtn  ...  attr5_3  sinc5_3  intel5_3  fun5_3  amb5_3\n",
              "0    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "1    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "2    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "3    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "4    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "\n",
              "[5 rows x 195 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljvW2uELCWqD",
        "colab_type": "text"
      },
      "source": [
        "5. Print out the shape of the dataframe (number of rows and columns) using the pandas attribute .shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ciEFAv5atrV",
        "colab_type": "code",
        "outputId": "fac2e0db-2255-427f-c4e5-0eae03397961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8378, 195)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOQFbN4vE8OU",
        "colab_type": "text"
      },
      "source": [
        "This dataset contains quite a lot of features (195) for 8378 rows. Let's check if there is any duplicated rows in it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsucQyinCgUa",
        "colab_type": "text"
      },
      "source": [
        "6. Print out the number of duplicated rows (looking at all columns from the dataframe) by combining the pandas methods .duplicated() and .sum():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqaDAiGzYrOS",
        "colab_type": "code",
        "outputId": "c9a3e5d8-69f1-4125-fa21-eb43f7a6ffb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.duplicated().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCODrawrFGC1",
        "colab_type": "text"
      },
      "source": [
        "Looking at the 195 columns of this dataset, there are no duplicate rows at all. Let's have an extra check but looking only at the identifiers variables listed in the dataset description document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQQfQrQeCsCw",
        "colab_type": "text"
      },
      "source": [
        "7. Print out the number of duplicated rows similarly to step 6 but this time look only at the identifiers columns ('iid','id','partner' and 'pid') by specifying the parameter 'subset':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4ctPmu4ZWJ8",
        "colab_type": "code",
        "outputId": "719bd67e-7d5c-4572-895f-e9581b2e0e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.duplicated(subset=['iid','id','partner','pid']).sum()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_xAkdNddkQP",
        "colab_type": "text"
      },
      "source": [
        "It seems there is no duplicated rows in this dataset.\n",
        "\n",
        "Looking at the dataset description document, we know the values of the following variables should range between 1 and 10: 'imprace', 'imprelig', 'sports', 'tvsports', 'exercise', 'dining',\n",
        "'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', \n",
        "'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga',\n",
        "'exphappy', 'satis_2'. In the next few steps, we are going to check there are no unexpected values for thes columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wjhBLxzD0nD",
        "colab_type": "text"
      },
      "source": [
        "8. Create a variable called 'scale_1_10' which will list the following columns names: 'imprace', 'imprelig', 'sports', 'tvsports', 'exercise', 'dining',\n",
        "'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', \n",
        "'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga',\n",
        "'exphappy', 'satis_2'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw6b3FrE6Z_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale_1_10 = ['imprace', 'imprelig', 'sports', 'tvsports', 'exercise', 'dining',\n",
        "'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', \n",
        "'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga',\n",
        "'exphappy', 'satis_2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTfNO9kwdynj",
        "colab_type": "text"
      },
      "source": [
        "10. Create a function called 'check_range' that takes as input parameter a column (a pandas Serie), minimum value and maximum value. The function will check for each row of the given column if it outside the given range (below minimum value or above the maximum value) and returns the corresponding list of binary value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9mY-J2rN1eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_range(column, min_value, max_value):\n",
        "  return (column < min_value) | (column >max_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jX7qTQeeXYY",
        "colab_type": "text"
      },
      "source": [
        "11. Test your function on the column 'imprace' and 1 and 10 respectively as the minimum and maximum values, save its output into a variable called 'unexpected_mask' and print its sum of to check how many cases are outside this range:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5aPwH8aUSvo",
        "colab_type": "code",
        "outputId": "37cd66de-62c1-4209-d510-f659d6d374f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unexpected_mask = check_range(df['imprace'], 1, 10)\n",
        "unexpected_mask.sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YRwNsHNfSze",
        "colab_type": "text"
      },
      "source": [
        "So there are 8 rows that have values for 'imprace' outside the expected range (between 1 and 10)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8agAHNiepqv",
        "colab_type": "text"
      },
      "source": [
        "12. Define a function called 'print_unexpected' that takes as input parameter a dataframe, column name, a list of binary values. This function will check the sum of the binary values is over 0 and if it is the case print out the column name, this sum and the unique values of the given column and the rows that matches the binary list (keeping only True values) using the pandas method '.loc' and .unique():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGvPiWPDTZXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_unexpected(df, col_name, unexpected_mask):\n",
        "  if unexpected_mask.sum() > 0:\n",
        "    print(col_name)\n",
        "    print(unexpected_mask.sum())\n",
        "    print(df.loc[unexpected_mask,col_name].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6Kp593YgQGa",
        "colab_type": "text"
      },
      "source": [
        "13. Test your function on the column 'imprace' with the output of the previous function, unexpected_mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIMHIsNqUjQ1",
        "colab_type": "code",
        "outputId": "3dc8feac-4b8f-4993-9721-378d1aa81a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print_unexpected(df, 'imprace', unexpected_mask)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imprace\n",
            "8\n",
            "[0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9pybyJagoHd",
        "colab_type": "text"
      },
      "source": [
        "We can see we still have 8 cases that are outside the expected range for this column and the unexpected value is 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EtAZBcPg0jR",
        "colab_type": "text"
      },
      "source": [
        "14. Create a function called 'check_ranges' that takes as input parameter a dataframe, a list of columns, a minimum and maximum values. This function will iterate through each column from the given column list, called the function 'check_range' and pass its output to the function 'print_unexpected' you defined in steps 10 and 12:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB4iQHewT4OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_ranges(df, col_list, min_value, max_value):\n",
        "  for col_name in col_list:\n",
        "    unexpected_mask = check_range(df[col_name], min_value, max_value)\n",
        "    print_unexpected(df, col_name, unexpected_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En669GLXhdMS",
        "colab_type": "text"
      },
      "source": [
        "15. Test this function with the dataset, the list 'scale_1_10' you defined at step 9 and 1 and 10 as their minimum and maximum values respectively:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqzkToaoUII-",
        "colab_type": "code",
        "outputId": "2c461c81-4964-4c5b-def0-dc88cb9ffd27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "check_ranges(df, scale_1_10, 1, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imprace\n",
            "8\n",
            "[0.]\n",
            "museums\n",
            "18\n",
            "[0.]\n",
            "art\n",
            "18\n",
            "[0.]\n",
            "hiking\n",
            "18\n",
            "[0.]\n",
            "gaming\n",
            "137\n",
            "[14.  0.]\n",
            "clubbing\n",
            "18\n",
            "[0.]\n",
            "reading\n",
            "51\n",
            "[13.]\n",
            "theater\n",
            "18\n",
            "[0.]\n",
            "movies\n",
            "18\n",
            "[0.]\n",
            "concerts\n",
            "18\n",
            "[0.]\n",
            "yoga\n",
            "36\n",
            "[0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upsBAUGWh1Il",
        "colab_type": "text"
      },
      "source": [
        "We can most of these columns have the unexpected value 0 and some of them have 13 and 14. In a real project, you will probably go and ask the surveyors if these values are expected or not. Let's say they confirmed the value 0 is actually a possible value in the survey but not 13 and 14 and they think it is just an error while they recorded these case and the value should be 10. Let's see how we can fix these issues in the next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J67a0ev8i4b4",
        "colab_type": "text"
      },
      "source": [
        "16. Create a function called 'replace_value' that takes as input parameter a dataframe, a column name, an incorrect value and a new value. This function will subset all the rows equals to the incorrect value for the given column and replace it with the new given value, print out the column name and the list of unique values of this column after replacement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIOoCo6zWqeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_value(df, col_name, incorrect_value, new_value):\n",
        "  df.loc[df[col_name] == incorrect_value, col_name] = new_value\n",
        "  print(col_name)\n",
        "  print(df[col_name].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiGQ5Hd_jZSz",
        "colab_type": "text"
      },
      "source": [
        "17. Test your function on the 'gaming' column, 14 as the incorrect value and 10 as the new value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5u1zYGQP0X_",
        "colab_type": "code",
        "outputId": "1f0fb0ac-2fd2-42ef-89f1-40fe7f488d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "replace_value(df, 'gaming', 14, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gaming\n",
            "[ 1.  5.  4.  6.  2.  3.  7.  8. 10. nan  9.  0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "288JaL1ZjkDu",
        "colab_type": "text"
      },
      "source": [
        "We see that after replacement the value 14 is not part of the possible values of this column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RALieVTvjtGO",
        "colab_type": "text"
      },
      "source": [
        "18. Use your function on the column 'reading', 13 as the incorrect value and 10 as the new value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hplI-CLuW_ms",
        "colab_type": "code",
        "outputId": "ebcf7df7-cbe2-475c-f709-a7da6ab9b0b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "replace_value(df, 'reading', 13, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading\n",
            "[ 6. 10.  7.  9.  8.  4.  5. nan  2.  3.  1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIqkb0XHj8DF",
        "colab_type": "text"
      },
      "source": [
        "We see that after replacement the value 13 is not part of the possible values of this column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iUHwNsTj-9d",
        "colab_type": "text"
      },
      "source": [
        "19. Create a for loop that will iterate through the following suffixes: ['1_1', '1_2', '1_3', '1_s', '2_1', '2_2', '2_3', '4_1', '4_2', '4_3', '7_2', '7_3']. For each of them, create a list comprehension (or another for loop) to extract the columns which contain the given suffix by using the method .endswith() and store them into a variable called 'suffix_cols' and then apply the function 'check_ranges' on this list and 0 and 100 as their minimum and maximum values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9TkNHqZSo03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for suffix in ['1_1', '1_2', '1_3', '1_s', '2_1', '2_2', '2_3', '4_1', '4_2', '4_3', '7_2', '7_3']:\n",
        "  suffix_cols = [col for col in df.columns if col.endswith(suffix)]\n",
        "  check_ranges(df, suffix_cols, 0, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tuCZIT-kzkz",
        "colab_type": "text"
      },
      "source": [
        "There is no output displayed that means all these columns have values within the expected range: between 0 and 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q3ukZIQk-3L",
        "colab_type": "text"
      },
      "source": [
        "20. Create a similar for loop as step 19 for the following suffixes and with 1 and 10 as minimum and maximum values: ['3_1', '3_2', '3_3', '5_1', '5_2', '5_3', '3_s']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8aBm1hTVK0Q",
        "colab_type": "code",
        "outputId": "0300069c-f8ae-4eb6-c9a9-f8d332ea5097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for suffix in ['3_1', '3_2', '3_3', '5_1', '5_2', '5_3', '3_s']:\n",
        "  suffix_cols = [col for col in df.columns if col.endswith(suffix)]\n",
        "  check_ranges(df, suffix_cols, 1, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attr3_3\n",
            "112\n",
            "[12.]\n",
            "sinc3_3\n",
            "173\n",
            "[12.]\n",
            "intel3_3\n",
            "233\n",
            "[12.]\n",
            "fun3_3\n",
            "153\n",
            "[12.]\n",
            "amb3_3\n",
            "147\n",
            "[12.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2KDy_8hlX4w",
        "colab_type": "text"
      },
      "source": [
        "We can see that all columns ending with '3_3' have 12 as unexpected values. Let's say after concertation with the surveyors we agreed to replace these values by 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YWZqQIklokM",
        "colab_type": "text"
      },
      "source": [
        "21. Create a for loop that iterates through the list of columns ending with '3_3' and call the function 'replace_values' for each of them and provide 12 as the incorect value and 10 as the new value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOmcJR8k6Kl8",
        "colab_type": "code",
        "outputId": "308ffb6c-d028-4a02-bfe8-1bb9aade4384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for col_name in ['attr3_3', 'sinc3_3', 'intel3_3', 'fun3_3', 'amb3_3']:\n",
        "  replace_value(df, col_name, 12, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attr3_3\n",
            "[ 5.  7. nan  6.  4.  9.  8.  3. 10.  2.]\n",
            "sinc3_3\n",
            "[ 7.  6. nan  5.  8.  9. 10.  4.  3.  2.]\n",
            "intel3_3\n",
            "[ 7.  9. nan  6. 10.  8.  5.  4.  3.]\n",
            "fun3_3\n",
            "[ 7.  9. nan  8.  6.  3.  5. 10.  2.  4.]\n",
            "amb3_3\n",
            "[ 7.  4. nan  5. 10.  9.  8.  6.  2.  3.  1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVCLkjmYbSt9",
        "colab_type": "text"
      },
      "source": [
        "Great! We have fixed the unexpected values for these columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laQfiDF-YLNM",
        "colab_type": "text"
      },
      "source": [
        "22. Print the data type of each variable using the attribute dtypes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv1a7YLL63I8",
        "colab_type": "code",
        "outputId": "b587f5c2-42a6-4b76-b842-4daf12e620af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "iid           int64\n",
              "id          float64\n",
              "gender        int64\n",
              "idg           int64\n",
              "condtn        int64\n",
              "             ...   \n",
              "attr5_3     float64\n",
              "sinc5_3     float64\n",
              "intel5_3    float64\n",
              "fun5_3      float64\n",
              "amb5_3      float64\n",
              "Length: 195, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h-Z1WMHmRzs",
        "colab_type": "text"
      },
      "source": [
        "We can see most of the columns have been detected as numerical variables but looking at the dataset description document, we know that they are categorical for most of them. Let's change their data type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwG02xuSmQZC",
        "colab_type": "text"
      },
      "source": [
        "23. Create a list called 'num_cols' containing the following list of columns: 'round', 'order', 'int_corr', 'age', 'mn_sat', 'income', 'expnum'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW87dAYWYp5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_cols = ['round', 'order', 'int_corr', 'age', 'mn_sat', 'income', 'expnum']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvx-aXJ-mrlv",
        "colab_type": "text"
      },
      "source": [
        "24. Create another list called 'cat_cols' containing the remaining columns names (excluding the ones in num_cols) of this dataframe using the attribute columns combined with the method '.difference()':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o25dBAOEdbv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_cols = df.columns.difference(num_cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoX9b4l0nEZw",
        "colab_type": "text"
      },
      "source": [
        "25. Create a for loop that will iterate through cat_cols and change the data type for each of them into a category using the method '.astype()':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AJNMy0Ec6ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col_name in cat_cols:\n",
        "  df[col_name] = df[col_name].astype('category')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1r8Yr0vnXj7",
        "colab_type": "text"
      },
      "source": [
        "26. Print the data type of each variable using the attribute dtypes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "973po6fCdCXa",
        "colab_type": "code",
        "outputId": "3a581ee8-c787-4515-d8ad-b1d31ce5a99d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "iid         category\n",
              "id          category\n",
              "gender      category\n",
              "idg         category\n",
              "condtn      category\n",
              "              ...   \n",
              "attr5_3     category\n",
              "sinc5_3     category\n",
              "intel5_3    category\n",
              "fun5_3      category\n",
              "amb5_3      category\n",
              "Length: 195, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LisLYckPnkYF",
        "colab_type": "text"
      },
      "source": [
        "Great! We have sorted out the data type for each column. Now let's see of we have missing columns for the numerical fields."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpDujixiYUtJ",
        "colab_type": "text"
      },
      "source": [
        "27. Print the number of missing values for each column in num_cols by combining the methods .isna() and .sum():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1a8onHi79Z7",
        "colab_type": "code",
        "outputId": "4eed3864-551b-4b97-b20c-1e7ed9c81352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "df[num_cols].isna().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "round          0\n",
              "order          0\n",
              "int_corr     158\n",
              "age           95\n",
              "mn_sat      5245\n",
              "income      4099\n",
              "expnum      6578\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_s11FSUn4t9",
        "colab_type": "text"
      },
      "source": [
        "There are some missing values for most of these columns. We need to fix these cases. Let's start with the column 'int_corr':"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE3-LZdZYdfK",
        "colab_type": "text"
      },
      "source": [
        "28. Print the unique values of the variable 'int_corr' using the method .unique():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPAAELUR9_qX",
        "colab_type": "code",
        "outputId": "4dd47d52-27f7-4847-907a-70acf24a6bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "df['int_corr'].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.14,  0.54,  0.16,  0.61,  0.21,  0.25,  0.34,  0.5 ,  0.28,\n",
              "       -0.36,  0.29,  0.18,  0.1 , -0.21,  0.32,  0.73,  0.6 ,  0.07,\n",
              "        0.11,  0.39, -0.24, -0.14,  0.09, -0.04, -0.3 , -0.26, -0.15,\n",
              "       -0.47, -0.18,  0.05,  0.37,  0.35,  0.15, -0.19, -0.43,  0.  ,\n",
              "       -0.17,  0.08, -0.16,  0.06, -0.05, -0.13, -0.06,  0.33, -0.51,\n",
              "        0.12,  0.19,  0.47,  0.03,  0.46,  0.43,  0.52, -0.46, -0.27,\n",
              "        0.59,  0.31, -0.34, -0.03, -0.11,  0.42, -0.4 , -0.23,  0.17,\n",
              "        0.68, -0.01, -0.35,  0.3 ,  0.65,  0.24,  0.41,  0.49,  0.01,\n",
              "        0.22, -0.08,  0.27,  0.44,  0.62, -0.2 , -0.02, -0.33, -0.52,\n",
              "       -0.1 ,  0.58, -0.57, -0.31, -0.07, -0.32,  0.04, -0.12,  0.48,\n",
              "       -0.22, -0.29,  0.38,  0.53, -0.38,  0.02, -0.28,  0.13,  0.2 ,\n",
              "         nan, -0.41, -0.44,  0.51, -0.48,  0.4 ,  0.26,  0.77, -0.49,\n",
              "       -0.25, -0.09,  0.45, -0.39,  0.83,  0.57, -0.61,  0.72, -0.37,\n",
              "        0.23, -0.58,  0.8 , -0.56,  0.63, -0.63,  0.71,  0.36,  0.56,\n",
              "        0.55,  0.76,  0.69,  0.79,  0.9 ,  0.67,  0.66,  0.81,  0.64,\n",
              "        0.74,  0.75,  0.85, -0.42, -0.5 , -0.59,  0.7 ,  0.82,  0.78,\n",
              "       -0.45, -0.83,  0.88, -0.7 , -0.62, -0.55,  0.87,  0.91,  0.84,\n",
              "       -0.64, -0.73, -0.54])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLNiGPaZYl-8",
        "colab_type": "text"
      },
      "source": [
        "The values of the column'int_corr' range between -1 and 1. It seems they have been normalised. As there are no extreme values or outliers, we can impute the missing values with the mean of this variable. This is what we are going to do in the next few steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfkMdBakZU9S",
        "colab_type": "text"
      },
      "source": [
        "29. Create a condition mask called int_corr_mask for finding the missing values in the column 'int_corr' using the method .isna():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zTWk7dtBbMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_corr_mask = df['int_corr'].isna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H57_8izQaR_0",
        "colab_type": "text"
      },
      "source": [
        "30. Display the number of missing values for this column using the method .sum() on 'int_corr_mask':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apH4JpnGBgNG",
        "colab_type": "code",
        "outputId": "7af4b021-660f-4e95-acd9-b443600864f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "int_corr_mask.sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl1l4IEWojwr",
        "colab_type": "text"
      },
      "source": [
        "We got the exact same number of missing values for 'int_corr' as in step 27."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WvdwLOOa3bJ",
        "colab_type": "text"
      },
      "source": [
        "31. Extract the mean of 'int_corr' using the method '.mean()' and store it in a new variable called int_corr_mean. Print its value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0wuvkFHBlml",
        "colab_type": "code",
        "outputId": "cc10c28e-0a1d-442f-aec9-73d453b5005f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "int_corr_mean = df['int_corr'].mean()\n",
        "print(int_corr_mean)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.19600973236009664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ATDFRTnbC0B",
        "colab_type": "text"
      },
      "source": [
        "The average value for this column is 0.196. We will replace all missing values by this value in the column 'int_corr'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBpzUxsEbNXk",
        "colab_type": "text"
      },
      "source": [
        "32. Replace all missing values from the variable 'int_corr' with its average using the method '.fillna()' with the parameter 'inplace=True': "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_m65ZKLB-jx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['int_corr'].fillna(int_corr_mean, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcelsCuRcMCM",
        "colab_type": "text"
      },
      "source": [
        "33. Print the number of missing values for 'int_corr' by combining the methods .isna() and .sum():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr6BfU45CDQw",
        "colab_type": "code",
        "outputId": "1b83cccc-2437-495a-e1db-a66a20ba6cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df['int_corr'].isna().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ACnGm6pcUsn",
        "colab_type": "text"
      },
      "source": [
        "Perfect! There is no mising value anymore in the variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxNwhfDAoSKh",
        "colab_type": "text"
      },
      "source": [
        "34. Create a new variable called 'missing_num_cols' containing the following columns: 'age', 'mn_sat', 'income', 'expnum'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSsH50CEZaDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "missing_num_cols = ['age', 'mn_sat', 'income', 'expnum']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obbUkigUcckI",
        "colab_type": "text"
      },
      "source": [
        "35. Create a for loop that will iterate through the columns in 'missing_num_cols' and print their name and their list of unique values using the method '.unique()':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SAofEd0CYdM",
        "colab_type": "code",
        "outputId": "a2fc952e-c090-43e6-8dca-285c985eed63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "for col_name in missing_num_cols:\n",
        "  print(col_name)\n",
        "  print(df[col_name].unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age\n",
            "[21. 24. 25. 23. 22. 26. 27. 30. 28. nan 29. 34. 35. 32. 39. 20. 19. 18.\n",
            " 37. 33. 36. 31. 42. 38. 55.]\n",
            "mn_sat\n",
            "[  nan 1070. 1258. 1400. 1290. 1460. 1430. 1215. 1330. 1450. 1155. 1140.\n",
            " 1360. 1402. 1250. 1210. 1220. 1410. 1260. 1380. 1030. 1309. 1308. 1050.\n",
            " 1100. 1310. 1490. 1188. 1097. 1212. 1340. 1034. 1185. 1242. 1160. 1099.\n",
            " 1214. 1270. 1110. 1178. 1060. 1157. 1180. 1014. 1341.  990. 1320. 1159.\n",
            " 1370. 1105. 1365. 1011. 1130. 1206. 1331. 1191.  914. 1200. 1080. 1090.\n",
            " 1092. 1470. 1149. 1134. 1230. 1267. 1280. 1227. 1239.]\n",
            "income\n",
            "[ 69487.  65929.     nan  37754.  86340.  60304.  54620.  48652.  29237.\n",
            "  56580.  36782.  38548.  52010.  28418.  43185.  23152.  43664.  48441.\n",
            "  61152.  36485.  41507.  17134.  30038.  33772.  24997.  42096.  28891.\n",
            "  62635.  12063.  29809.  26482.  30147.  39919.  41466.  23988.  28989.\n",
            "  50948.  38022.  47559.  53539.  32159.  53940.  40753.  38207.  46166.\n",
            "  30973.  28317.  26645.  25589.  55223. 109031.  40409.  21597.  76624.\n",
            "  35968.  51725.  55419.  55550.  26682.  41547.  23361.  74893.  52804.\n",
            "  53923.  27094.  57213.  42390.  43636.  57887.  30768.  66699.  45360.\n",
            "  55080.  17378.  40375.  48929.  78193.  63351.  50745.  29279.  38774.\n",
            "  58802.  41831.  52186.  97857.  74624.  21590.  38832.  37248.  28240.\n",
            "  53771.  56096.  31560.  52467.  80006.  47572.  22439.  31383.  40749.\n",
            "  47997.  78704.  31143.  32129.  44195.  46837.  97972.  35960.  65708.\n",
            "  49466.  53229.  32649.  35867.  40244.  42640.  52388.  62875.  30855.\n",
            "  46800.  45695.  46792.  53501.  64716.  27248.  22805.  56118.  30146.\n",
            "  39123.  46153.  45300.  42397.  44346.  42225.  37405.  28524.  61141.\n",
            "   8607.  41476.  49841.  37240.  36594.  62997.  46608.  37881.  48944.\n",
            "  77112.  18283.  31432.  73073.  26706.  50060.  25401.  80608.  43844.\n",
            "  53196.  25786.  39394.  40695.  45788.  37315.  51663.  32563.  54303.\n",
            "  16908.  39729.  57316.  30587.  57513.  31857.  23207.  25831.  28759.\n",
            "  19264.  41778.  35963.  49409.  31516.  36223.  43367.  27503.  35187.\n",
            "  26298.  31148.  55704.  46138.  66827.  42897.  31809.  75347.  47005.\n",
            "  52805.  50725.  65693.  45736.  33906.  50501.  48785.  52318.  62844.\n",
            "  52586.  29236.  31486.  31632. 106663.  84043.  35224.  36381.  65498.\n",
            "  60000.  22669.  81266.  29746.  47556.  42651.  27794.  41737.  90225.\n",
            "  52280.  56056.  60835.  62829.  16767.  42967.  21488.  89977.  18619.\n",
            "  22161.  82734.  40163.  46185.  78844.  29575.  34752.  22173.  37994.\n",
            "  35409.  23707.  57501.  25314.  48876.  34870.  35848.  45017.  12416.\n",
            "  87789.  50572.  49642.  20000.  32508.  35627.  46280.  41191.  71787.\n",
            "  72412.  36510.  32386.  15863.  46272.  48137.  61686.  47624.  36673.\n",
            "  55138.]\n",
            "expnum\n",
            "[ 2.  5. 10.  3. 15. 20.  4.  9. 19.  0. nan  8. 12.  1.  7.  6. 18. 14.\n",
            " 13.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo4SyqvfqF9X",
        "colab_type": "text"
      },
      "source": [
        "The values for these columns are not normalised and some of them have outliers so this time we are going to use their median to fill in the missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9yMwAtTct5M",
        "colab_type": "text"
      },
      "source": [
        "36. Create a for loop similar to step 35 but this time you will calculate the median of each column and save it into a variable called 'col_median', impute missing values with this median value using the method '.fillna()' with the parameter 'inplace=True', print the name of the column and its median value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH3Qd9LIHeKF",
        "colab_type": "code",
        "outputId": "5abd6c1d-7ab3-4666-ff71-9f64c9cff05b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "for col_name in missing_num_cols:\n",
        "  col_median = df[col_name].median()\n",
        "  df[col_name].fillna(col_median, inplace=True)\n",
        "  print(col_name)\n",
        "  print(col_median)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age\n",
            "26.0\n",
            "mn_sat\n",
            "1310.0\n",
            "income\n",
            "43185.0\n",
            "expnum\n",
            "4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7IzimWUcve5",
        "colab_type": "text"
      },
      "source": [
        "37. Create a for loop similar to step 35 but this time you will print the name of each column and their number of missing values using the combination of the methods '.isna()' and '.sum()':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-CLjjkVHrfq",
        "colab_type": "code",
        "outputId": "689dd0c5-26a6-4f81-cec6-11553a71dab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "for col_name in missing_num_cols:\n",
        "  print(col_name)\n",
        "  print(df[col_name].isna().sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age\n",
            "0\n",
            "mn_sat\n",
            "0\n",
            "income\n",
            "0\n",
            "expnum\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YlExsvyZp4P",
        "colab_type": "text"
      },
      "source": [
        "Excellent! In this activity we have cleaned most of the main quality issues for this dataset. We looked for duplication, incorrect values, wrong data types and missing values. You have also put in practice all the techniques we learned in this chapter to fix these issues. We are now more confident in using this modified version of the dataset to the next step of the project if this was a real use case."
      ]
    }
  ]
}